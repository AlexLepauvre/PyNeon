{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading a Neon dataset/recording\n",
    "In this tutorial, we will show how to load a single Neon recording downloaded from [Pupil Cloud](https://docs.pupil-labs.com/neon/pupil-cloud/).\n",
    "\n",
    "## Reading sample data\n",
    "We will use a sample recording produced by the NCC Lab called `OfficeWalk`. It's a project with 2 recordings and multiple enrichments and can be downloaded with the `get_sample_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from pyneon import get_sample_data, NeonDataset, NeonRecording\n",
    "\n",
    "sample_dir = get_sample_data(\"OfficeWalk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OfficeWalk` data has the following structure:\n",
    "\n",
    "```plaintext\n",
    "OfficeWalk\n",
    "├── Timeseries Data\n",
    "│   ├── walk1-e116e606\n",
    "│   │   ├── info.json\n",
    "│   │   ├── gaze.csv\n",
    "│   │   └── ....\n",
    "│   ├── walk2-93b8c234\n",
    "│   │   ├── info.json\n",
    "│   │   ├── gaze.csv\n",
    "│   │   └── ....\n",
    "|   ├── enrichment_info.txt\n",
    "|   └── sections.csv\n",
    "├── OfficeWalk_FACE-MAPPER_FaceMap\n",
    "├── OfficeWalk_MARKER-MAPPER_TagMap_csv\n",
    "└── OfficeWalk_STATIC-IMAGE-MAPPER_ManualMap_csv\n",
    "```\n",
    "\n",
    "The `Timeseries Data` folder contains what PyNeon calls a `NeonDataset`. It contains multiple recordings, each with its own `info.json` file and data files. These recordings can either be loaded individually as `NeonRecording`s or as a wholist `NeonDataset`.\n",
    "\n",
    "If loading a `NeonDataset`, specify the path to the `Timeseries Data` folder to create a `NeonDataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = sample_dir / \"Timeseries Data\"\n",
    "dataset = NeonDataset(dataset_dir)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeonDataset has a `recordings` attribute that contains a list of `NeonRecording` objects. These `NeonRecording` objects can be accessed by their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_recording = dataset[0]\n",
    "print(type(first_recording))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can directly load a single `NeonRecording` by specifying the path to the recording's folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_dir = dataset_dir / \"walk1-e116e606\"\n",
    "recording = NeonRecording(recording_dir)\n",
    "print(type(recording))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and metadata of a NeonRecording\n",
    "An overview of basic metadata and contents of a `NeonRecording` can be obtained by printing the object. An initiated `NeonRecording` locates data files in the recording directory but does not load them until requested to be memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual data streams can be accessed as properties of the `NeonRecording` object. For example, the gaze data can be accessed as `recording.gaze`, and upon accessing, the tabular data is loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"recording._gaze size before accessing `gaze`: {sys.getsizeof(recording._gaze)}\")\n",
    "\n",
    "gaze = recording.gaze\n",
    "print(f\"recording.gaze is of type: {type(gaze)}\")\n",
    "print(f\"recording._gaze size after accessing `gaze`: {sys.getsizeof(recording._gaze)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the timeseries data in the gaze stream as a pandas DataFrame by accessing the `data` attribute of the gaze stream. The columns of the DataFrame include `timestamp [ns]` and channel data columns. During loading, PyNeon strips the redundant `section id` and `recording id` columns and adds a more human-readable `time [s]` column to represent the time of each sample in seconds relative to the start of the data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gaze.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyNeon also automatically sets the column datatype to appropriate types, such as `Int64` for timestamps, `Int32` for event IDs, and `float64` for float data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gaze.data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streams and events\n",
    "\n",
    "Up to this point, PyNeon simply reads and re-organizes the raw .csv files. Let's plot some samples from the `gaze` and `eye_states` streams and a saccade from the `saccades` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gaze_color = \"royalblue\"\n",
    "gyro_color = \"darkorange\"\n",
    "\n",
    "imu = recording.imu\n",
    "saccades = recording.saccades\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax2 = ax.twinx()\n",
    "ax.yaxis.label.set_color(gaze_color)\n",
    "ax2.yaxis.label.set_color(gyro_color)\n",
    "\n",
    "# Visualize the 2nd saccade\n",
    "saccade = saccades.data.iloc[1]\n",
    "ax.axvspan(\n",
    "    saccade[\"start timestamp [ns]\"], saccade[\"end timestamp [ns]\"], color=\"lightgray\"\n",
    ")\n",
    "ax.text(\n",
    "    (saccade[\"start timestamp [ns]\"] + saccade[\"end timestamp [ns]\"]) / 2,\n",
    "    1050,\n",
    "    \"Saccade\",\n",
    "    horizontalalignment=\"center\",\n",
    ")\n",
    "\n",
    "# Visualize gaze x and pupil diameter left\n",
    "sns.scatterplot(\n",
    "    ax=ax,\n",
    "    data=gaze.data.head(100),\n",
    "    x=\"timestamp [ns]\",\n",
    "    y=\"gaze x [px]\",\n",
    "    color=gaze_color,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    ax=ax2,\n",
    "    data=imu.data.head(60),\n",
    "    x=\"timestamp [ns]\",\n",
    "    y=\"gyro x [deg/s]\",\n",
    "    color=gyro_color,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's apparent that at the beginning of the recording, there are some missing data points in both the `gaze` and `imu` streams. This is presumably due to the time it takes for the sensors to start up and stabilize. We will show how to handle missing data using resampling in the next tutorial. For now, it's important to be aware of these gaps and that it will require great caution to assume the data is continuously and equally sampled.\n",
    "\n",
    "PyNeon also calculates the effective (as opposed to the nominal) sampling frequency of each stream by dividing the number of samples by the duration of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Gaze: nominal sampling frequency = {gaze.sampling_freq_nominal}, \"\n",
    "    f\"effective sampling frequency = {gaze.sampling_freq_effective}\"\n",
    ")\n",
    "print(\n",
    "    f\"IMU: nominal sampling frequency = {recording.imu.sampling_freq_nominal}, \"\n",
    "    f\"effective sampling frequency = {recording.imu.sampling_freq_effective}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing gaze heatmap\n",
    "Finally, we will show how to plot a heatmap of the gaze/fixation data. We will further add violin plots to show the distribution in x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import gridspec\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Generate the heatmap data\n",
    "x_edges = np.linspace(0, 1600, 160)\n",
    "y_edges = np.linspace(0, 1200, 120)\n",
    "\n",
    "# Sample gaze and fixation data (replace with your actual data)\n",
    "gaze_x = gaze.data[\"gaze x [px]\"]\n",
    "gaze_y = gaze.data[\"gaze y [px]\"]\n",
    "gaze_heatmap, _, _ = np.histogram2d(gaze_x, gaze_y, bins=(x_edges, y_edges))\n",
    "gaze_heatmap = gaussian_filter(gaze_heatmap, sigma=3)\n",
    "\n",
    "fixations = recording.fixations\n",
    "fix_x = fixations.data[\"fixation x [px]\"]\n",
    "fix_y = fixations.data[\"fixation y [px]\"]\n",
    "\n",
    "# Create a figure with gridspec layout\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(\n",
    "    4, 4, width_ratios=[0.2, 1, 1, 0.2], height_ratios=[0.2, 1, 1, 0.2]\n",
    ")\n",
    "\n",
    "# Main heatmap plot\n",
    "ax_main = fig.add_subplot(gs[1:3, 1:3])\n",
    "ax_main.imshow(\n",
    "    gaze_heatmap.T, cmap=\"inferno\", extent=[0, 1600, 0, 1200], origin=\"lower\"\n",
    ")\n",
    "ax_main.scatter(fix_x, fix_y, color=\"white\", s=10, alpha=0.3)\n",
    "ax_main.set_aspect(\"equal\", \"box\")\n",
    "ax_main.set_xlabel(\"Scene camera x [px]\")\n",
    "ax_main.set_ylabel(\"Scene camera y [px]\")\n",
    "\n",
    "# Violin plot for x distribution (top)\n",
    "ax_top = fig.add_subplot(gs[0, 1:3], sharex=ax_main)\n",
    "ax_top.violinplot(fix_x, vert=False, showmedians=True)\n",
    "ax_top.axis(\"off\")  # Hide x-axis labels and ticks\n",
    "\n",
    "# Violin plot for y distribution (right)\n",
    "ax_right = fig.add_subplot(gs[1:3, 3], sharey=ax_main)\n",
    "ax_right.violinplot(fix_y, vert=True, showmedians=True)\n",
    "ax_right.axis(\"off\")  # Hide y-axis labels and ticks\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can neatly see that the recorded data shows a centre-bias, which is a well-known effect from eye statistics. In y, we can see that fixations tend to occur below the horizon, which is indicative of a walking task where a participant looks at the floor in front of them more often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyneon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
